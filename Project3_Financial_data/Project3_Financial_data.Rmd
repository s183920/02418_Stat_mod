---
title: 'Project3: Financial data'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Packages

```{r Packages, include = FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(GGally)
library(magrittr)
```


# Data

## Load
```{r}
finance_data <- read.csv("../Data/finance_data.csv", header = T, sep = ";") %>% 
  # tidyr::separate("time", c("year", "month", "day"), sep = "-") %>% 
  dplyr::mutate(time = lubridate::ymd(time)) 
  
summary(finance_data)

```

#Part 1

We wish to visualise the data and estimate the parameters of a normal model aswell as adress if the normal model is appropriate. We can plot the data to look at it to see a big ocean of
```{r}
## Look at data
plot(finance_data$time,finance_data$SLV)

qqnorm(finance_data$SLV)
qqline(finance_data$SLV)

## design matrix, and obs
X <- cbind(1,finance_data$time)
y <- finance_data$SLV
n <- length(y)
#X
#y

glm(y~X, data = finance_data)

#muhat <- mean(y)
#sigma.hat2 <- sum((y-muhat)^2)/n

#MLE
muhat = mean(y)
sigma.hat2 <- 1/n*sum((y-mean(y))^2)
#muhat
#sigma.hat2
#var(y)
s2 <- var(y) * (n - 1)/n 
#s2
normal.ll <- sum(dnorm(y, mean = mean(y),
                       sd = sqrt(s2), log = TRUE))
cat("\nMLE\n")
cat("Muhat:",mean(y),"\n")
cat("Sigmahat2:",sigma.hat2,"\n")

cat("\nConfidence interval with 95% confidence:",c(muhat + 1.96*sqrt(s2)/sqrt(n),muhat - 1.96*sqrt(s2)/sqrt(n)),"\n")

cat("\nNormal log likelihood using Jan's solution:", normal.ll,"\n")


#AIC of the normal model
cat("AIC of Jans model:",-2 * normal.ll + 4,"\n")
sigma2 <- sigma.hat2

#The solution from the equation in the book is propably too general. The solution provided by Jan in his examples prints the same AIC when computed as using R's normal "gml" function, which makes us belive that we will have to fit the normal model using the dnorm instead of trying to write out the equation from the book. (pg. 90)

logl <- function(y,n){
  -n/2*log(sigma2)-1/(2*sigma2)*sum((y-mean(y))^2)
}

cat("Normal log likelihood using the equation in the book:",logl(y,n),"\n")
cat("AIC of my model:",-2 * logl(y,n) + 4,"\n")


# ## correlation between estimates
# cov2cor(t(X)%*%X)
# 
# X[ ,2] <- X[ ,2] - mean(X[ ,2])
# cov2cor(t(X)%*%X)
# 
# ## Parameter estimaes
# beta <- solve(t(X) %*% X) %*% t(X) %*% y
# beta
# 
# ## variance parameter
# yhat <- X %*% beta
# sigmasq.hat <- sum((y-yhat)^2)/n
# 
# ## Unbiased estimate
# 
# sigmasq.hat
# sigmasq.hat2
# 
# se.beta <- sqrt(diag(sigmasq.hat2 * solve(t(X) %*% X)))
# se.beta
```
```{r}
profile_mu_normal <- function(theta, data){
    # The profile likelihood of the location parameter of the cauchy model.
    # The MLE for the scale parameter is found numerically at each fixed value of the location parameter using nlminb
    nlogL_normal_scale <- function(theta, loc, data){
      # Defininf negative log likelihood with fixed location parameter
        scale <- theta[1]
        -sum(dnorm(data, loc, scale, log = T))
        }
  scale = nlminb(c(0), nlogL_normal_scale, lower = c(0.001), data = data, loc = theta[1])$par
  
  # print(scale)
  sum(dnorm(data, theta, scale, log = T))
}
opt_norm <- optimize(profile_mu_normal, c(-5,5), data = y, maximum = T)
loc <- seq(-0.05,0.05, 0.0001)
p_norm_profile <- c()

for (i in 1:length(loc)){
  
  p_norm_profile[i] = profile_mu_normal(loc[i], y)
}
normalised_LL <-  p_norm_profile-opt_norm$objective
plot(loc,normalised_LL, 'l', main = "Normalised Log likelihodd", xlab = "Location parameter", ylab = "Log likelihodd")
plot(loc, exp(normalised_LL), 'l',  main = "Normalised likelihodd", xlab = "Location parameter", ylab = "Likelihodd")
```

Looking at the QQ plot, we see to much noise in the ends of the data. With this many datapoints, we cannot deem this model appropriate when the tails deviate this much from the QQline. We must therefore look at another model.

## Part 2. 

See section 4.11 "Location-scale family"
We consider the Cauchy model since it "is useful as a model for data with heavy tails, characterized by the presence of outliers".
The model is complex, which calls for numerical optimisation methods.
```{r}
logL_cauchy <- function(theta, data){
  # The negative log Likelihood function for the cauchy model
  # Cauchy is a multiparameter model in the location scale family, with 2 parameters:
  # location (loc)
  # scale (scale)
  # It is useful for modelling distributions with heavy tails

  loc <- theta[1]
  scale <- theta[2]
  -sum(dcauchy(data, loc, scale, log = T))
}

# Using the numerical nlminb optimizer to minimise the negative log likelihood function
opt <- nlminb(c(0,0.00001), logL_cauchy, lower = c(-Inf, 0.0000001), data = y)


profile_mu_cauchy <- function(theta, data){
    # The profile likelihood of the location parameter of the cauchy model.
    # The MLE for the scale parameter is found numerically at each fixed value of the location parameter using nlminb
    nlogL_cauchy_scale <- function(theta, loc, data){
      # Defininf negative log likelihood with fixed location parameter
        scale <- theta[1]
        -sum(dcauchy(data, loc, scale, log = T))
        }
  scale = nlminb(c(0.00001), nlogL_cauchy_scale, lower = c(0.0000001), data = data, loc = theta[1])$par

  sum(dcauchy(data, theta[1], scale, log = T))
}

# Plotting the profile log likelihood of the location parameter
opt_profile_cauchy <- optimize(profile_mu_cauchy, interval = c(-10,10), data = y, maximum = T)
loc <- seq(-0.05,0.05, 0.0001)
p_cauchy <- c()

for (i in 1:length(loc)){
  
  p_cauchy[i] = profile_mu_cauchy(loc[i], y)
}
normalised_LL_C <- p_cauchy-opt_profile_cauchy$objective 
plot(loc,normalised_LL_C , 'l', main = "Normalised Log likelihodd", xlab = "Location parameter", ylab = "Log likelihodd")
plot(loc, exp(normalised_LL_C), 'l', main = "Normalised likelihood",xlab = "Location parameter", ylab = "Likelihodd")

# Optimal MLE of location parameter with profile likelihood

```

### Comparing Cauchy and Normal

```{r}
plot(loc, exp(normalised_LL), 'l')
lines(loc, exp(normalised_LL_C), lty = 3, col ="red")
alpha <- 0.05
c <- exp(-1/2*qchisq(1-alpha, df = 1))
abline( h = c)
p = 2
AIC_normal = -2 * normal.ll + 4
AIC_cauchy = 2 *opt$objective + 4

cat("AIC of the normal model: ", AIC_normal,"  AIC of the Cauchy model: ", AIC_cauchy)
```
AIC is lowest for the normal model - indicating the normal to be a better fit. 
We try the t distribution with density:
$$p(x \mid \nu, \hat{\mu}, \hat{\sigma})=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right) \sqrt{\pi \nu} \hat{\sigma}}\left(1+\frac{1}{\nu}\left(\frac{x-\hat{\mu}}{\hat{\sigma}}\right)^{2}\right)^{-\frac{\nu+1}{2}}$$
```{r}
t_dist_Profile <- function(theta, df, data){
    mu <- theta[1]
    s  <- theta[2]
    -sum(log(gamma((df+1)/2)/(gamma(df/2) * sqrt(pi * df)*s) * (1 + 1/df*(((data-mu)/s)^2))^(-(df+1)/2)))
}
opt <- nlminb(c(0,0.000001), t_dist_Profile, lower = c(-Inf, 0.000001), data = y, df = 4)

df <- seq(1,30,1)
result <- c()
for (i in 1:length(df)){
  opt <- nlminb(c(0,0.000001), t_dist_Profile, lower = c(-Inf, 0.000001), data = y, df = i)
  result[i] <- -opt$objective
}

maximum <- df[result == max(result)]
plot(df, result, main = "Log likelikhood as a function of degrees of freedom", xlab = "Degrees of freedom", ylab = "Log likelihood")
cat("The maximum of the profile likelihood of df is found at", maximum, "degrees of freedom")

opt_max <- nlminb(c(0,0.000001), t_dist_Profile, lower = c(-Inf, 0.000001), data = y, df = maximum)
cat("The parameters for the optimal model are found at", maximum, "degrees of freedom - mu = ", opt_max$par[1], "sigma = ", opt_max$par[2])

AIC_T <- 2*opt_max$objective +2*3

print("Comparing AIC")
cat("Normal: ", AIC_normal, "\nCauchy: ", AIC_cauchy, "\nT: ", AIC_T)
```


